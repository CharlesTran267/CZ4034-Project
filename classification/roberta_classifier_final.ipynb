{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import gensim.downloader\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "from torchsummary import summary\n",
    "\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = pd.read_csv('data/incomplete_annotations_data2.csv')\n",
    "\n",
    "# annotated_data = full_data[full_data['Subjectivity'].notnull()]\n",
    "# unannotated_data = full_data[full_data['Subjectivity'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subjectivity\n",
       "1.0    1560\n",
       "0.0    1123\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_data['Subjectivity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Polarity\n",
       "1.0    876\n",
       "0.0    684\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_data['Polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "- Lowercasing\n",
    "- Removing stopwords\n",
    "- Replacing emoji and slang/abbreviations with their text counterparts\n",
    "\n",
    "- Mispellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Search Term</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Source</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity 2</th>\n",
       "      <th>Polarity 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>JW Anderson</td>\n",
       "      <td>JW Anderson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>{'Likes_and_timestamp': '0 likes on 2023-11-20...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brand  Search Term Comment     Source  \\\n",
       "2590  JW Anderson  JW Anderson     NaN  Instagram   \n",
       "\n",
       "                                               Metadata  Subjectivity  \\\n",
       "2590  {'Likes_and_timestamp': '0 likes on 2023-11-20...           0.0   \n",
       "\n",
       "      Polarity  Subjectivity 2  Polarity 2  \n",
       "2590       NaN               1           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_data[annotated_data['Comment'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand              103\n",
      "Search Term        174\n",
      "Comment              0\n",
      "Source               0\n",
      "Metadata           234\n",
      "Subjectivity         0\n",
      "Polarity          1122\n",
      "Subjectivity 2       0\n",
      "Polarity 2           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "annotated_data = annotated_data.dropna(subset=['Comment'])\n",
    "print(annotated_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ain't\": 'is not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he he will have', \"he's\": 'he is', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"i'd\": 'i would', \"i'd've\": 'i would have', \"i'll\": 'i will', \"i'll've\": 'i will have', \"i'm\": 'i am', \"i've\": 'i have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is', \"there'd\": 'there would', \"there'd've\": 'there would have', \"there's\": 'there is', \"they'd\": 'they would', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you'll've\": 'you will have', \"you're\": 'you are', \"you've\": 'you have', '$': ' dollar ', '€': ' euro ', '4ao': 'for adults only', 'a.m': 'before midday', 'a3': 'anytime anywhere anyplace', 'aamof': 'as a matter of fact', 'acct': 'account', 'adih': 'another day in hell', 'afaic': 'as far as i am concerned', 'afaict': 'as far as i can tell', 'afaik': 'as far as i know', 'afair': 'as far as i remember', 'afk': 'away from keyboard', 'app': 'application', 'approx': 'approximately', 'apps': 'applications', 'asap': 'as soon as possible', 'asl': 'age, sex, location', 'atk': 'at the keyboard', 'ave.': 'avenue', 'aymm': 'are you my mother', 'ayor': 'at your own risk', 'b&b': 'bed and breakfast', 'b+b': 'bed and breakfast', 'b.c': 'before christ', 'b2b': 'business to business', 'b2c': 'business to customer', 'b4': 'before', 'b4n': 'bye for now', 'b@u': 'back at you', 'bae': 'before anyone else', 'bak': 'back at keyboard', 'bbbg': 'bye bye be good', 'bbc': 'british broadcasting corporation', 'bbias': 'be back in a second', 'bbl': 'be back later', 'bbs': 'be back soon', 'be4': 'before', 'bfn': 'bye for now', 'blvd': 'boulevard', 'bout': 'about', 'brb': 'be right back', 'bros': 'brothers', 'brt': 'be right there', 'bsaaw': 'big smile and a wink', 'btw': 'by the way', 'bwl': 'bursting with laughter', 'c/o': 'care of', 'cet': 'central european time', 'cf': 'compare', 'cia': 'central intelligence agency', 'csl': 'can not stop laughing', 'cu': 'see you', 'cul8r': 'see you later', 'cv': 'curriculum vitae', 'cwot': 'complete waste of time', 'cya': 'see you', 'cyt': 'see you tomorrow', 'dae': 'does anyone else', 'dbmib': 'do not bother me i am busy', 'diy': 'do it yourself', 'dm': 'direct message', 'dwh': 'during work hours', 'e123': 'easy as one two three', 'eet': 'eastern european time', 'eg': 'example', 'embm': 'early morning business meeting', 'encl': 'enclosed', 'encl.': 'enclosed', 'etc': 'and so on', 'faq': 'frequently asked questions', 'fawc': 'for anyone who cares', 'fb': 'facebook', 'fc': 'fingers crossed', 'fig': 'figure', 'fimh': 'forever in my heart', 'ft.': 'feet', 'ft': 'featuring', 'ftl': 'for the loss', 'ftw': 'for the win', 'fwiw': 'for what it is worth', 'fyi': 'for your information', 'g9': 'genius', 'gahoy': 'get a hold of yourself', 'gal': 'get a life', 'gcse': 'general certificate of secondary education', 'gfn': 'gone for now', 'gg': 'good game', 'gl': 'good luck', 'glhf': 'good luck have fun', 'gmt': 'greenwich mean time', 'gmta': 'great minds think alike', 'gn': 'good night', 'g.o.a.t': 'greatest of all time', 'goat': 'greatest of all time', 'goi': 'get over it', 'gps': 'global positioning system', 'gr8': 'great', 'gratz': 'congratulations', 'gyal': 'girl', 'h&c': 'hot and cold', 'hp': 'horsepower', 'hr': 'hour', 'hrh': 'his royal highness', 'ht': 'height', 'ibrb': 'i will be right back', 'ic': 'i see', 'icq': 'i seek you', 'icymi': 'in case you missed it', 'idc': 'i do not care', 'idgadf': 'i do not give a damn fuck', 'idgaf': 'i do not give a fuck', 'idk': 'i do not know', 'ie': 'that is', 'i.e': 'that is', 'ifyp': 'i feel your pain', 'IG': 'instagram', 'iirc': 'if i remember correctly', 'ilu': 'i love you', 'ily': 'i love you', 'imho': 'in my humble opinion', 'imo': 'in my opinion', 'imu': 'i miss you', 'iow': 'in other words', 'irl': 'in real life', 'j4f': 'just for fun', 'jic': 'just in case', 'jk': 'just kidding', 'jsyk': 'just so you know', 'l8r': 'later', 'lb': 'pound', 'lbs': 'pounds', 'ldr': 'long distance relationship', 'lmao': 'laugh my ass off', 'lmfao': 'laugh my fucking ass off', 'lol': 'laughing out loud', 'ltd': 'limited', 'ltns': 'long time no see', 'm8': 'mate', 'mf': 'motherfucker', 'mfs': 'motherfuckers', 'mfw': 'my face when', 'mofo': 'motherfucker', 'mph': 'miles per hour', 'mr': 'mister', 'mrw': 'my reaction when', 'ms': 'miss', 'mte': 'my thoughts exactly', 'nagi': 'not a good idea', 'nbc': 'national broadcasting company', 'nbd': 'not big deal', 'nfs': 'not for sale', 'ngl': 'not going to lie', 'nhs': 'national health service', 'nrn': 'no reply necessary', 'nsfl': 'not safe for life', 'nsfw': 'not safe for work', 'nth': 'nice to have', 'nvr': 'never', 'nyc': 'new york city', 'oc': 'original content', 'og': 'original', 'ohp': 'overhead projector', 'oic': 'oh i see', 'omdb': 'over my dead body', 'omg': 'oh my god', 'omw': 'on my way', 'p.a': 'per annum', 'p.m': 'after midday', 'pm': 'prime minister', 'poc': 'people of color', 'pov': 'point of view', 'pp': 'pages', 'ppl': 'people', 'prw': 'parents are watching', 'ps': 'postscript', 'pt': 'point', 'ptb': 'please text back', 'pto': 'please turn over', 'qpsa': 'what happens', 'ratchet': 'rude', 'rbtl': 'read between the lines', 'rlrt': 'real life retweet', 'rofl': 'rolling on the floor laughing', 'roflol': 'rolling on the floor laughing out loud', 'rotflmao': 'rolling on the floor laughing my ass off', 'rt': 'retweet', 'ruok': 'are you ok', 'sfw': 'safe for work', 'sk8': 'skate', 'smh': 'shake my head', 'sq': 'square', 'srsly': 'seriously', 'ssdd': 'same stuff different day', 'tbh': 'to be honest', 'tbs': 'tablespooful', 'tbsp': 'tablespooful', 'tfw': 'that feeling when', 'thks': 'thank you', 'tho': 'though', 'thx': 'thank you', 'tia': 'thanks in advance', 'til': 'today i learned', 'tl;dr': 'too long i did not read', 'tldr': 'too long i did not read', 'tmb': 'tweet me back', 'tntl': 'trying not to laugh', 'ttyl': 'talk to you later', 'u': 'you', 'u2': 'you too', 'u4e': 'yours for ever', 'utc': 'coordinated universal time', 'w/': 'with', 'w/o': 'without', 'w8': 'wait', 'wassup': 'what is up', 'wb': 'welcome back', 'wtf': 'what the fuck', 'wtg': 'way to go', 'wtpa': 'where the party at', 'wuf': 'where are you from', 'wuzup': 'what is up', 'wywh': 'wish you were here', 'yd': 'yard', 'ygtr': 'you got that right', 'ynk': 'you never know', 'zzz': 'sleeping bored and tired'}\n"
     ]
    }
   ],
   "source": [
    "with open('abbreviations_list.pkl', 'rb') as file:\n",
    "    abbreviations = pickle.load(file)\n",
    "\n",
    "print(abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating extra column for preprocessed text\n",
    "annotated_data['Preprocessed Comment'] = annotated_data['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing emojis\n",
    "import emoji\n",
    "\n",
    "def demojize_with_delimiters(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "annotated_data['Preprocessed Comment'] = annotated_data['Preprocessed Comment'].apply(lambda x: demojize_with_delimiters(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "\n",
    "annotated_data['Preprocessed Comment'] = annotated_data['Preprocessed Comment'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Louis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Louis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Ensure the input is a string\n",
    "    if isinstance(text, str):\n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Get the list of stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Remove stopwords from the tokenized words\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        \n",
    "        # Join the filtered words back into a single string\n",
    "        filtered_text = ' '.join(filtered_words)\n",
    "        \n",
    "        return filtered_text\n",
    "    else:  \n",
    "        return text\n",
    "\n",
    "annotated_data['Preprocessed Comment'] = annotated_data['Preprocessed Comment'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to manually tokenize text including punctuations\n",
    "def custom_tokenize(text):\n",
    "    # Regex pattern to match words (including contractions) and separate punctuation\n",
    "    tokens = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    return tokens\n",
    "\n",
    "# Normalize slangs and abbreviations\n",
    "def normalize_slangs_abbreviations_custom(text, slang_dict):\n",
    "    if isinstance(text, str):\n",
    "        tokens = custom_tokenize(text)\n",
    "        normalized_tokens = [slang_dict.get(token.lower(), token) for token in tokens]\n",
    "        # Reconstruct the text\n",
    "        normalized_text = ' '.join(normalized_tokens).replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" !\", \"!\").replace(\" ?\", \"?\")\n",
    "        return normalized_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "annotated_data['Preprocessed Comment'] = annotated_data['Preprocessed Comment'].apply(lambda x: normalize_slangs_abbreviations_custom(x, abbreviations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotated_data = full_data[full_data['Subjectivity'].notnull()]\n",
    "# unannotated_data = full_data[full_data['Subjectivity'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Search Term</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Source</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity 2</th>\n",
       "      <th>Polarity 2</th>\n",
       "      <th>Preprocessed Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>waste</td>\n",
       "      <td>Designing products with sustainability in mind...</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>{'Name': 'Angla Sicurella', 'Handle': '@AnglaS...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>designing products sustainability mind, like n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>waste</td>\n",
       "      <td>Kirby would have been a waste of time - why ev...</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>{'Name': 'LisaKingWheless', 'Handle': '@Lisapc...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>kirby would waste time even ask? plus adds coa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>waste</td>\n",
       "      <td>I wouldn’t spend another dollar at that theate...</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>{'Name': 'Sheila McSheilerton', 'Handle': '@sh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>spend another dollar theater. like buy nike gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>waste</td>\n",
       "      <td>Call them back and tell them they’re lying bec...</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>{'Name': 'UncleChrissy', 'Handle': '@uncle_chr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>call back tell lying already. trying get real ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>waste</td>\n",
       "      <td>I’m really sitting here going in on myself..li...</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>{'Name': 'Jade ☥', 'Handle': '@jmerarity', 'Ti...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>really sitting going.. like really going let b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678</th>\n",
       "      <td>Louis Vuitton</td>\n",
       "      <td>Louis Vuitton</td>\n",
       "      <td>❤️❤️❤️</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>{'Likes_and_timestamp': '0 likes on 2024-01-17...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>red_heart red_heart red_heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2679</th>\n",
       "      <td>Tory Burch</td>\n",
       "      <td>Tory Burch</td>\n",
       "      <td>The pale pink in the 6th look is EVERYTHINGGGG...</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>{'Likes_and_timestamp': '0 likes on 2023-09-16...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>pale pink 6th look everythinggggg. cherry_blossom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680</th>\n",
       "      <td>Yeezy</td>\n",
       "      <td>Yeezy</td>\n",
       "      <td>He said it himself this isn't the real Kanye s...</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>{'Likes_and_timestamp': '0 likes on 2024-02-27...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>said n't real kanye care imposter saying face_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>Gucci</td>\n",
       "      <td>Gucci</td>\n",
       "      <td>😍😍😍</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>{'Likes_and_timestamp': '3 likes on 2023-09-23...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>smiling_face_with_heart eyes smiling_face_with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>Chanel, Miu Miu, Versace</td>\n",
       "      <td>Chanel, Miu Miu, Versace</td>\n",
       "      <td>Anyone here who can guide me</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>{'Likes_and_timestamp': '2 likes on 2024-03-10...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anyone guide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2682 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Brand               Search Term  \\\n",
       "0                         Nike                     waste   \n",
       "1                         Nike                     waste   \n",
       "2                         Nike                     waste   \n",
       "3                         Nike                     waste   \n",
       "4                         Nike                     waste   \n",
       "...                        ...                       ...   \n",
       "2678             Louis Vuitton             Louis Vuitton   \n",
       "2679                Tory Burch                Tory Burch   \n",
       "2680                     Yeezy                     Yeezy   \n",
       "2681                     Gucci                     Gucci   \n",
       "2682  Chanel, Miu Miu, Versace  Chanel, Miu Miu, Versace   \n",
       "\n",
       "                                                Comment     Source  \\\n",
       "0     Designing products with sustainability in mind...    Twitter   \n",
       "1     Kirby would have been a waste of time - why ev...    Twitter   \n",
       "2     I wouldn’t spend another dollar at that theate...    Twitter   \n",
       "3     Call them back and tell them they’re lying bec...    Twitter   \n",
       "4     I’m really sitting here going in on myself..li...    Twitter   \n",
       "...                                                 ...        ...   \n",
       "2678                                             ❤️❤️❤️  Instagram   \n",
       "2679  The pale pink in the 6th look is EVERYTHINGGGG...  Instagram   \n",
       "2680  He said it himself this isn't the real Kanye s...  Instagram   \n",
       "2681                                                😍😍😍  Instagram   \n",
       "2682                       Anyone here who can guide me  Instagram   \n",
       "\n",
       "                                               Metadata  Subjectivity  \\\n",
       "0     {'Name': 'Angla Sicurella', 'Handle': '@AnglaS...           0.0   \n",
       "1     {'Name': 'LisaKingWheless', 'Handle': '@Lisapc...           1.0   \n",
       "2     {'Name': 'Sheila McSheilerton', 'Handle': '@sh...           1.0   \n",
       "3     {'Name': 'UncleChrissy', 'Handle': '@uncle_chr...           1.0   \n",
       "4     {'Name': 'Jade ☥', 'Handle': '@jmerarity', 'Ti...           1.0   \n",
       "...                                                 ...           ...   \n",
       "2678  {'Likes_and_timestamp': '0 likes on 2024-01-17...           1.0   \n",
       "2679  {'Likes_and_timestamp': '0 likes on 2023-09-16...           1.0   \n",
       "2680  {'Likes_and_timestamp': '0 likes on 2024-02-27...           0.0   \n",
       "2681  {'Likes_and_timestamp': '3 likes on 2023-09-23...           1.0   \n",
       "2682  {'Likes_and_timestamp': '2 likes on 2024-03-10...           0.0   \n",
       "\n",
       "      Polarity  Subjectivity 2  Polarity 2  \\\n",
       "0          NaN               1           0   \n",
       "1          0.0               1           1   \n",
       "2          0.0               1           1   \n",
       "3          0.0               1           1   \n",
       "4          1.0               0           0   \n",
       "...        ...             ...         ...   \n",
       "2678       1.0               0           0   \n",
       "2679       1.0               1           1   \n",
       "2680       NaN               1           0   \n",
       "2681       1.0               1           1   \n",
       "2682       NaN               0           0   \n",
       "\n",
       "                                   Preprocessed Comment  \n",
       "0     designing products sustainability mind, like n...  \n",
       "1     kirby would waste time even ask? plus adds coa...  \n",
       "2     spend another dollar theater. like buy nike gr...  \n",
       "3     call back tell lying already. trying get real ...  \n",
       "4     really sitting going.. like really going let b...  \n",
       "...                                                 ...  \n",
       "2678                      red_heart red_heart red_heart  \n",
       "2679  pale pink 6th look everythinggggg. cherry_blossom  \n",
       "2680  said n't real kanye care imposter saying face_...  \n",
       "2681  smiling_face_with_heart eyes smiling_face_with...  \n",
       "2682                                       anyone guide  \n",
       "\n",
       "[2682 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHY is Hermes even getting involved at the Lotus casino, seems like a damn waste of time – tho I know they're probably trying to give Luke more backstory before the finale\n",
      "hermes even getting involved lotus casino, seems like damn waste time though know 're probably trying give luke backstory finale\n"
     ]
    }
   ],
   "source": [
    "print(annotated_data['Comment'].iloc[20])\n",
    "print(annotated_data['Preprocessed Comment'].iloc[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjectivity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_texts = annotated_data['Comment'].tolist()\n",
    "annotated_labels = annotated_data['Polarity'].tolist()\n",
    "\n",
    "# Tokenize texts using BERT tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "encoded_data = tokenizer(annotated_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Extract attention masks\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(annotated_labels)\n",
    "\n",
    "# Split the annotated data into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels, train_masks, val_masks = train_test_split(encoded_data['input_ids'], labels, attention_masks, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define DataLoader for training and validation sets\n",
    "train_dataset = TensorDataset(train_texts, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_texts, val_masks, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─RobertaModel: 1-1                      --\n",
      "|    └─RobertaEmbeddings: 2-1            --\n",
      "|    |    └─Embedding: 3-1               38,603,520\n",
      "|    |    └─Embedding: 3-2               394,752\n",
      "|    |    └─Embedding: 3-3               768\n",
      "|    |    └─LayerNorm: 3-4               1,536\n",
      "|    |    └─Dropout: 3-5                 --\n",
      "|    └─RobertaEncoder: 2-2               --\n",
      "|    |    └─ModuleList: 3-6              85,054,464\n",
      "├─RobertaClassificationHead: 1-2         --\n",
      "|    └─Linear: 2-3                       590,592\n",
      "|    └─Dropout: 2-4                      --\n",
      "|    └─Linear: 2-5                       1,538\n",
      "=================================================================\n",
      "Total params: 124,647,170\n",
      "Trainable params: 124,647,170\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─RobertaModel: 1-1                      --\n",
       "|    └─RobertaEmbeddings: 2-1            --\n",
       "|    |    └─Embedding: 3-1               38,603,520\n",
       "|    |    └─Embedding: 3-2               394,752\n",
       "|    |    └─Embedding: 3-3               768\n",
       "|    |    └─LayerNorm: 3-4               1,536\n",
       "|    |    └─Dropout: 3-5                 --\n",
       "|    └─RobertaEncoder: 2-2               --\n",
       "|    |    └─ModuleList: 3-6              85,054,464\n",
       "├─RobertaClassificationHead: 1-2         --\n",
       "|    └─Linear: 2-3                       590,592\n",
       "|    └─Dropout: 2-4                      --\n",
       "|    └─Linear: 2-5                       1,538\n",
       "=================================================================\n",
       "Total params: 124,647,170\n",
       "Trainable params: 124,647,170\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize BERT model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "model.to(device)\n",
    "summary(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "best_val_loss = float('inf')  # Initialize with positive infinity\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "\n",
    "no_improvement_count = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)  # No need to pass labels here\n",
    "        logits = outputs.logits\n",
    "\n",
    "        labels = labels.long()\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels)  # Compute cross-entropy loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)  # No need to pass labels during evaluation\n",
    "            logits = outputs.logits\n",
    "\n",
    "            labels = labels.long()\n",
    "\n",
    "            val_loss += F.cross_entropy(logits, labels).item()\n",
    "            \n",
    "            val_preds.extend(torch.argmax(logits, dim=1).tolist())\n",
    "            val_targets.extend(labels.tolist())\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = sum(1 for p, t in zip(val_preds, val_targets) if p == t) / len(val_preds)\n",
    "\n",
    "    precision = precision_score(val_targets, val_preds)\n",
    "    recall = recall_score(val_targets, val_preds)\n",
    "    f1 = f1_score(val_targets, val_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}, Val Precision: {precision}, Val Recall: {recall}, Val F1: {f1}\")\n",
    "\n",
    "    # Update best validation loss and accuracy\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_val_precision = precision\n",
    "        best_val_recall = recall\n",
    "        best_val_f1 = f1\n",
    "\n",
    "        best_epoch_loss = epoch + 1\n",
    "        no_improvement_count = 0\n",
    "\n",
    "        best_model_path = 'models/best_roberta_adamw_subjectivity2.pth'\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    if no_improvement_count >= patience:\n",
    "        print(f\"No improvement for {patience} epochs. Early stopping...\")\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Best Validation Loss: {best_val_loss} at Epoch {best_epoch_loss}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy} at Epoch {best_epoch_loss}\")\n",
    "print(f\"Best Validation Precision: {best_val_precision} at Epoch {best_epoch_loss}\")\n",
    "print(f\"Best Validation Recall: {best_val_recall} at Epoch {best_epoch_loss}\")\n",
    "print(f\"Best Validation F1: {best_val_f1} at Epoch {best_epoch_loss}\")\n",
    "\n",
    "print(f\"Time taken to train the model: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Validation Loss: 0.5012361325469672 at Epoch 3\n",
    "- Best Validation Accuracy: 0.7763975155279503 at Epoch 3\n",
    "- Best Validation Precision: 0.8590078328981723 at Epoch 3\n",
    "- Best Validation Recall: 0.7230769230769231 at Epoch 3\n",
    "- Best Validation F1: 0.7852028639618138 at Epoch 3\n",
    "- Time taken to train the model: 35304.90 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Louis\\Anaconda3\\envs\\inforetrieval\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 69/69 [38:17<00:00, 33.29s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 45.723883748054504, Val Loss: 0.662416026989619, Val Accuracy: 0.5662393162393162, Val Precision: 0.5577342047930284, Val Recall: 1.0, Val F1: 0.7160839160839161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 69/69 [37:17<00:00, 32.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 42.73330116271973, Val Loss: 0.5174851556619008, Val Accuracy: 0.7735042735042735, Val Precision: 0.7906976744186046, Val Recall: 0.796875, Val F1: 0.7937743190661478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 69/69 [37:21<00:00, 32.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 31.557389616966248, Val Loss: 0.4945437297224998, Val Accuracy: 0.811965811965812, Val Precision: 0.7781456953642384, Val Recall: 0.91796875, Val F1: 0.8422939068100358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 69/69 [37:19<00:00, 32.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 24.202057898044586, Val Loss: 0.47943010727564495, Val Accuracy: 0.7692307692307693, Val Precision: 0.8303571428571429, Val Recall: 0.7265625, Val F1: 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 69/69 [37:20<00:00, 32.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 22.492990363389254, Val Loss: 0.49837560604015985, Val Accuracy: 0.7692307692307693, Val Precision: 0.7283950617283951, Val Recall: 0.921875, Val F1: 0.8137931034482758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 69/69 [37:18<00:00, 32.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 18.950306314975023, Val Loss: 0.5018869072198868, Val Accuracy: 0.8012820512820513, Val Precision: 0.8146718146718147, Val Recall: 0.82421875, Val F1: 0.8194174757281554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 69/69 [37:16<00:00, 32.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 20.494622353464365, Val Loss: 0.5270366872350375, Val Accuracy: 0.7692307692307693, Val Precision: 0.743421052631579, Val Recall: 0.8828125, Val F1: 0.8071428571428572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 69/69 [37:17<00:00, 32.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 10.931127285584807, Val Loss: 0.8575857601128518, Val Accuracy: 0.7414529914529915, Val Precision: 0.8857142857142857, Val Recall: 0.60546875, Val F1: 0.7192575406032483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 69/69 [37:18<00:00, 32.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 9.367206977214664, Val Loss: 0.6195476113508145, Val Accuracy: 0.8076923076923077, Val Precision: 0.8051470588235294, Val Recall: 0.85546875, Val F1: 0.8295454545454546\n",
      "No improvement for 5 epochs. Early stopping...\n",
      "Best Validation Loss: 0.47943010727564495 at Epoch 4\n",
      "Best Validation Accuracy: 0.7692307692307693 at Epoch 4\n",
      "Best Validation Precision: 0.8303571428571429 at Epoch 4\n",
      "Best Validation Recall: 0.7265625 at Epoch 4\n",
      "Best Validation F1: 0.775 at Epoch 4\n",
      "Time taken to train the model: 21695.17 seconds\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the annotated data (assuming it has columns 'text' and 'polarity')\n",
    "annotated_polarity_data = annotated_data[annotated_data['Subjectivity']==1]\n",
    "\n",
    "annotated_texts = annotated_polarity_data['Comment'].tolist()\n",
    "annotated_labels = annotated_polarity_data['Polarity'].tolist()\n",
    "\n",
    "# Tokenize texts using BERT tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "encoded_data = tokenizer(annotated_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Extract attention masks\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(annotated_labels)\n",
    "\n",
    "# Split the annotated data into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels, train_masks, val_masks = train_test_split(encoded_data['input_ids'], labels, attention_masks, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define DataLoader for training and validation sets\n",
    "train_dataset = TensorDataset(train_texts, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_texts, val_masks, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize BERT model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "best_val_loss = float('inf')  # Initialize with positive infinity\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "\n",
    "no_improvement_count = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)  # No need to pass labels here\n",
    "        logits = outputs.logits\n",
    "\n",
    "        labels = labels.long()\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels)  # Compute cross-entropy loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)  # No need to pass labels during evaluation\n",
    "            logits = outputs.logits\n",
    "\n",
    "            labels = labels.long()\n",
    "\n",
    "            val_loss += F.cross_entropy(logits, labels).item()\n",
    "            \n",
    "            val_preds.extend(torch.argmax(logits, dim=1).tolist())\n",
    "            val_targets.extend(labels.tolist())\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = sum(1 for p, t in zip(val_preds, val_targets) if p == t) / len(val_preds)\n",
    "\n",
    "    precision = precision_score(val_targets, val_preds)\n",
    "    recall = recall_score(val_targets, val_preds)\n",
    "    f1 = f1_score(val_targets, val_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}, Val Precision: {precision}, Val Recall: {recall}, Val F1: {f1}\")\n",
    "\n",
    "    # Update best validation loss and accuracy\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_val_precision = precision\n",
    "        best_val_recall = recall\n",
    "        best_val_f1 = f1\n",
    "\n",
    "        best_epoch_loss = epoch + 1\n",
    "        no_improvement_count = 0\n",
    "\n",
    "        best_model_path = 'models/best_roberta_adamw_polarity2.pth'\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    if no_improvement_count >= patience:\n",
    "        print(f\"No improvement for {patience} epochs. Early stopping...\")\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Best Validation Loss: {best_val_loss} at Epoch {best_epoch_loss}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy} at Epoch {best_epoch_loss}\")\n",
    "print(f\"Best Validation Precision: {best_val_precision} at Epoch {best_epoch_loss}\")\n",
    "print(f\"Best Validation Recall: {best_val_recall} at Epoch {best_epoch_loss}\")\n",
    "print(f\"Best Validation F1: {best_val_f1} at Epoch {best_epoch_loss}\")\n",
    "\n",
    "print(f\"Time taken to train the model: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inforetrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
